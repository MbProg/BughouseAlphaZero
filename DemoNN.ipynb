{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pickle\n",
    "import zarr\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, BatchNormalization, Activation\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "class RL_Datapoint():\n",
    "    def __init__(self, state, policy, values):\n",
    "        self.state = state\n",
    "        self.policy = policy\n",
    "        self.values = values\n",
    "        \n",
    "def __read_file_data(ID, path='dataset/'):\n",
    "    dataset = zarr.group(store=zarr.ZipStore(path + str(ID) +'.zip', mode=\"r\"))\n",
    "    X = np.array(dataset['states'])\n",
    "    policies = np.array(dataset['policies'])\n",
    "    values = np.array(dataset['values'])\n",
    "    return X,policies,values\n",
    "\n",
    "def generator(batch_size, datasetFileLength, path='dataset/'):\n",
    "    while True:\n",
    "        files_sequence = list(range(datasetFileLength))\n",
    "        np.random.shuffle(files_sequence)\n",
    "\n",
    "        for file_step, file_id in enumerate(files_sequence): \n",
    "            X, policies, values = __read_file_data(file_id,path)\n",
    "#             X = np.rollaxis(X, 1,4).shape\n",
    "            rand_indices = np.arange(len(X))\n",
    "            np.random.shuffle(rand_indices)\n",
    "\n",
    "            for i in range(int(len(X)/batch_size)):\n",
    "                batch_indices = rand_indices[i*batch_size: (i+1)*batch_size]\n",
    "                yield X[batch_indices],[policies[batch_indices],values[batch_indices]]\n",
    "def plotHistory( history):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_policy_loss = history.history['val_policy_loss']\n",
    "    val_value_loss = history.history['val_value_loss']\n",
    "    loss = history.history['loss']\n",
    "    policy_loss = history.history['policy_loss']\n",
    "    value_loss = history.history['value_loss']\n",
    "    \n",
    "    epochs = range(1,len(loss) + 1)\n",
    "\n",
    "    # fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs,loss,'bo',label='loss')\n",
    "    plt.plot(epochs,val_loss,'b',label='val_loss')\n",
    "    plt.title = 'Training and validation loss'\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs,policy_loss,'bo',label='policy_loss')\n",
    "    plt.plot(epochs,val_policy_loss,'b',label='val_policy_loss')\n",
    "    plt.title = 'Training and validation policy loss'\n",
    "    plt.legend()        \n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs,value_loss,'bo',label='value_loss')\n",
    "    plt.plot(epochs,val_value_loss,'b',label='val_value_loss')\n",
    "    plt.title = 'Training and validation value loss'\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResidualNetwork(input_shape, output_value=1, output_policy=2272):\n",
    "    \n",
    "    channel_pos = 'channels_first'\n",
    "    inp_shape = Input(input_shape,name='input1')\n",
    "    x = Conv2D(256, kernel_size=(3,3), padding = 'same', input_shape=input_shape,data_format=channel_pos,name='conv2d_Prep')(inp_shape)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_prep')(x)\n",
    "    x_a1 = Activation('relu',name='activation_prep')(x)\n",
    "    activated_x = x_a1\n",
    "    \n",
    "#     activated_x, x\n",
    "    def createResidualBlock(block_nr, activated_x):\n",
    "        nr = block_nr *2 -1\n",
    "        x = Conv2D(256, kernel_size=(3,3), name = 'conv2d_'+str(nr), padding='same',data_format=channel_pos)(activated_x)\n",
    "        x = BatchNormalization(axis=1, name = 'batch_normalization_'+str(nr))(x)\n",
    "        x = Activation('relu',name = 'activation_'+str(nr))(x)\n",
    "        x = Conv2D(256, kernel_size=(3,3), name = 'conv2d_'+str(nr+1),padding = 'same',data_format=channel_pos)(x)\n",
    "        x = BatchNormalization(axis=1, name = 'batch_normalization_'+str(nr+1))(x)\n",
    "        x = keras.layers.add([x,activated_x],name='add_' + str(block_nr))\n",
    "        activated_x = Activation('relu',name='activation_'+str(nr+1))(x)\n",
    "        return activated_x\n",
    "    \n",
    "    # build eight residual blocks\n",
    "    for i in range (1,8):\n",
    "        activated_x = createResidualBlock(i, activated_x)\n",
    "        \n",
    "\n",
    "    \n",
    "    # Value header\n",
    "    x = Conv2D(1, kernel_size=(1,1),name='value_conv2d', padding = 'same',data_format=channel_pos)(activated_x)\n",
    "    xb = BatchNormalization(axis=1,name='value_batch_normalization')(x)\n",
    "    xA = Activation('relu',name='value_activation')(xb)\n",
    "    xF = Flatten(name='value_flatten')(xA)\n",
    "    dense_1 = Dense(256, activation='relu',name='value_dense')(xF)\n",
    "    value = Dense(output_value, activation='tanh', name='value')(dense_1)\n",
    "\n",
    "    # Policy Header\n",
    "    xConv = Conv2D(8, kernel_size=(7,7), padding = 'same',name='policy_conv2d',data_format=channel_pos)(activated_x)\n",
    "    xb = BatchNormalization(axis=1,name='policy_batch_normalization')(xConv)\n",
    "    xA = Activation('relu',name='policy_activation')(xb)\n",
    "    xF = Flatten(name='policy_flatten')(xA)\n",
    "    policy = Dense(output_policy, activation='softmax', name='policy')(xF)\n",
    "\n",
    "\n",
    "    from keras.models import Model\n",
    "    model = Model(inp_shape, [policy,value])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NN Architecture: ResNet</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Main Parameters of the NN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ids: 0  -  899\n",
      "val_ids: 900  -  999\n",
      "x_val: (10000, 60, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "# ----------- parameters ----------------\n",
    "files_len = 1000                                # amount of zip files\n",
    "file_ids = np.arange(files_len)               \n",
    "# np.random.shuffle(file_ids)\n",
    "train_ids = file_ids[:int(len(file_ids)*0.9)] # file_ids for training set\n",
    "val_ids = file_ids[int(len(file_ids)*0.9):]   # file_ids for validation set \n",
    "filepath=\"models/model-{epoch:02d}.hdf5\"      # path to save model\n",
    "zip_length = 10000                            # amount of datapoints in a zip file\n",
    "data_len = files_len * zip_length             # whole amount of datapoints in all zip files\n",
    "batch_size = 256                              # batch size\n",
    "steps_per_epoch = int((len(train_ids)*zip_length)/batch_size) # amount of batches in one epoch\n",
    "CLASSES_LEN = 2272                            # amount of classes for policy\n",
    "channel_pos = 'channels_last'          \n",
    "dataset_path = 'dataReal/train/'                     # relative directory to the dataset\n",
    "inp_shape = (60,8,8)                          # shape of a datapoint\n",
    "val_id = files_len -1 \n",
    "x_val, policies_val, values_val = __read_file_data(val_id,path=dataset_path)\n",
    "print('train_ids:',np.min(train_ids),' - ', np.max(train_ids))\n",
    "print('val_ids:',np.min(val_ids),' - ', np.max(val_ids))\n",
    "print('x_val:', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1 (InputLayer)             (None, 60, 8, 8)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_Prep (Conv2D)            (None, 256, 8, 8)    138496      input1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_prep (Batch (None, 256, 8, 8)    1024        conv2d_Prep[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_prep (Activation)    (None, 256, 8, 8)    0           batch_normalization_prep[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 8, 8)    590080      activation_prep[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 8, 8)    1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 8, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 8, 8)    590080      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 8, 8)    1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256, 8, 8)    0           batch_normalization_2[0][0]      \n",
      "                                                                 activation_prep[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 8, 8)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 8, 8)    590080      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256, 8, 8)    1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256, 8, 8)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 8, 8)    590080      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 8, 8)    1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256, 8, 8)    0           batch_normalization_4[0][0]      \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256, 8, 8)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 256, 8, 8)    590080      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256, 8, 8)    1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256, 8, 8)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 256, 8, 8)    590080      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256, 8, 8)    1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256, 8, 8)    0           batch_normalization_6[0][0]      \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 256, 8, 8)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 256, 8, 8)    590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 256, 8, 8)    1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 256, 8, 8)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 256, 8, 8)    590080      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256, 8, 8)    1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256, 8, 8)    0           batch_normalization_8[0][0]      \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256, 8, 8)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 256, 8, 8)    590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256, 8, 8)    1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 256, 8, 8)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 256, 8, 8)    590080      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256, 8, 8)    1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256, 8, 8)    0           batch_normalization_10[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 256, 8, 8)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 8, 8)    590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 256, 8, 8)    1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 256, 8, 8)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 8, 8)    590080      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 256, 8, 8)    1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 256, 8, 8)    0           batch_normalization_12[0][0]     \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 256, 8, 8)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 8, 8)    590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 256, 8, 8)    1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 256, 8, 8)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 8, 8)    590080      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256, 8, 8)    1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 256, 8, 8)    0           batch_normalization_14[0][0]     \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 256, 8, 8)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "value_conv2d (Conv2D)           (None, 1, 8, 8)      257         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "policy_conv2d (Conv2D)          (None, 8, 8, 8)      100360      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "value_batch_normalization (Batc (None, 1, 8, 8)      4           value_conv2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "policy_batch_normalization (Bat (None, 8, 8, 8)      32          policy_conv2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "value_activation (Activation)   (None, 1, 8, 8)      0           value_batch_normalization[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "policy_activation (Activation)  (None, 8, 8, 8)      0           policy_batch_normalization[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "value_flatten (Flatten)         (None, 64)           0           value_activation[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "policy_flatten (Flatten)        (None, 512)          0           policy_activation[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "value_dense (Dense)             (None, 256)          16640       value_flatten[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "policy (Dense)                  (None, 2272)         1165536     policy_flatten[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 1)            257         value_dense[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 9,698,062\n",
      "Trainable params: 9,690,364\n",
      "Non-trainable params: 7,698\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getResidualNetwork(inp_shape, output_policy=CLASSES_LEN)\n",
    "sgd = optimizers.SGD(lr=0.000, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "def acc_reg(y_true,y_pred):\n",
    "    return K.constant(1) - K.square(K.mean((y_pred-y_true), axis=1))\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=sgd,\n",
    "              metrics=['accuracy'], loss_weights=[0.999,0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define callbacks</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Callback for checkpoint and tensorboard</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks \n",
    "from datetime import datetime\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "logdir=\"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Callback for Learning rate</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchLen:  195310  - DataLen:  10000000\n"
     ]
    }
   ],
   "source": [
    "# learning rate\n",
    "from  LearningRateScheduler import *\n",
    "epochs = 5\n",
    "batch_len = epochs * int(data_len/ (batch_size))\n",
    "max_lr = 0.001*8\n",
    "total_it = batch_len\n",
    "min_lr = 0.0001\n",
    "print('BatchLen: ', batch_len, ' - DataLen: ', data_len)\n",
    "lr_schedule = OneCycleSchedule(start_lr=max_lr/8, max_lr=max_lr, cycle_length=total_it*.4, cooldown_length=total_it*.6, finish_lr=min_lr)\n",
    "scheduler = LinearWarmUp(lr_schedule, start_lr=min_lr, length=total_it/30)\n",
    "bt = BatchLearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint,tensorboard_callback,bt]\n",
    "# callbacks_list = [checkpoint,bt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_first\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "backend.set_image_dim_ordering('th')\n",
    "print(backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    2/35156 [..............................] - ETA: 28:41:45 - loss: 7.7773 - policy_loss: 7.7850 - value_loss: 0.0794 - policy_acc: 0.0000e+00 - value_acc: 0.8984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoBray\\.conda\\envs\\TensorFlow_GPU_Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (2.612980). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\MoBray\\.conda\\envs\\TensorFlow_GPU_Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.314450). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35156/35156 [==============================] - 6597s 188ms/step - loss: 3.0065 - policy_loss: 3.0095 - value_loss: 0.0329 - policy_acc: 0.4106 - value_acc: 0.9914 - val_loss: 2.3496 - val_policy_loss: 2.3519 - val_value_loss: 0.0166 - val_policy_acc: 0.4507 - val_value_acc: 0.9998\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.34960, saving model to models/model-01.hdf5\n",
      "Epoch:  1  - lr: 0.006133369  - batch: 0  - epoch:  1\n",
      "Epoch 2/5\n",
      "11731/35156 [=========>....................] - ETA: 1:15:16 - loss: 1.8444 - policy_loss: 1.8462 - value_loss: 0.0140 - policy_acc: 0.5313 - value_acc: 0.9999"
     ]
    }
   ],
   "source": [
    "# model.save('models\\\\BughouseNet220620190437.h5')\n",
    "history = model.fit_generator(generator(batch_size,len(train_ids),path=dataset_path), steps_per_epoch=int((len(train_ids)*zip_length)/batch_size), callbacks=callbacks_list,\n",
    "                    epochs=epochs, validation_data=(x_val, [policies_val,values_val]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  3/351 [..............................] - ETA: 6:45 - loss: 7.9823 - policy_loss: 7.9902 - value_loss: 0.0819 - policy_acc: 0.0000e+00 - policy_acc_reg: 1.0000 - value_acc: 0.9023 - value_acc_reg: 0.9181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoBray\\.conda\\envs\\TensorFlow_GPU_Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.362056). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 71s 204ms/step - loss: 6.3754 - policy_loss: 6.3817 - value_loss: 0.0642 - policy_acc: 0.0608 - policy_acc_reg: 1.0000 - value_acc: 0.9555 - value_acc_reg: 0.9358 - val_loss: 5.7280 - val_policy_loss: 5.7337 - val_value_loss: 0.0760 - val_policy_acc: 0.0772 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9543 - val_value_acc_reg: 0.9240\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.72800, saving model to models/model-01.hdf5\n",
      "Epoch:  1  - lr: 0.0014083333  - batch: 0  - epoch:  1\n",
      "Epoch 2/20\n",
      "351/351 [==============================] - 69s 195ms/step - loss: 5.5165 - policy_loss: 5.5219 - value_loss: 0.0729 - policy_acc: 0.1007 - policy_acc_reg: 1.0000 - value_acc: 0.9459 - value_acc_reg: 0.9271 - val_loss: 5.8568 - val_policy_loss: 5.8627 - val_value_loss: 0.0466 - val_policy_acc: 0.0708 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9953 - val_value_acc_reg: 0.9534\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 5.72800\n",
      "Epoch:  2  - lr: 0.0029833333  - batch: 1  - epoch:  2\n",
      "Epoch 3/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 5.1806 - policy_loss: 5.1857 - value_loss: 0.0681 - policy_acc: 0.1495 - policy_acc_reg: 1.0000 - value_acc: 0.9451 - value_acc_reg: 0.9319 - val_loss: 6.1780 - val_policy_loss: 6.1841 - val_value_loss: 0.0567 - val_policy_acc: 0.1295 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9668 - val_value_acc_reg: 0.9433\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.72800\n",
      "Epoch:  3  - lr: 0.004558333  - batch: 2  - epoch:  3\n",
      "Epoch 4/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 4.8144 - policy_loss: 4.8192 - value_loss: 0.0788 - policy_acc: 0.2046 - policy_acc_reg: 1.0000 - value_acc: 0.9267 - value_acc_reg: 0.9212 - val_loss: 4.9401 - val_policy_loss: 4.9449 - val_value_loss: 0.1037 - val_policy_acc: 0.2031 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.8685 - val_value_acc_reg: 0.8963\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.72800 to 4.94010, saving model to models/model-04.hdf5\n",
      "Epoch:  4  - lr: 0.0061333333  - batch: 3  - epoch:  4\n",
      "Epoch 5/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 4.3064 - policy_loss: 4.3106 - value_loss: 0.0711 - policy_acc: 0.2673 - policy_acc_reg: 1.0000 - value_acc: 0.9374 - value_acc_reg: 0.9289 - val_loss: 4.9451 - val_policy_loss: 4.9500 - val_value_loss: 0.0176 - val_policy_acc: 0.1734 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9945 - val_value_acc_reg: 0.982466 - policy_loss: 4.3609 - value_loss: 0.0748 - policy_acc: 0.2622 - policy_acc_reg: 1.0000 - value_acc: 0.9306 - val - ETA: 18s - loss: 4.3505 - policy_loss: 4.3548 - value_loss: 0.0741 - policy_acc: 0.2630 - policy_acc_reg: 1.0000 - value_acc: - ETA: 12s - loss: 4.3363 - policy_loss: 4.3406 - value_loss: 0.0726 - policy_acc: 0.2634 - policy_acc_reg: 1.0000 - value_acc: 0.9346 - val - ETA: 9s - loss: 4.3263 - policy_loss: 4.3305 - value_lo\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.94010\n",
      "Epoch:  5  - lr: 0.0077083334  - batch: 4  - epoch:  5\n",
      "Epoch 6/20\n",
      "351/351 [==============================] - 69s 197ms/step - loss: 3.8932 - policy_loss: 3.8970 - value_loss: 0.0648 - policy_acc: 0.3074 - policy_acc_reg: 1.0000 - value_acc: 0.9515 - value_acc_reg: 0.9352 - val_loss: 4.1673 - val_policy_loss: 4.1714 - val_value_loss: 0.0539 - val_policy_acc: 0.2554 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9682 - val_value_acc_reg: 0.9461\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.94010 to 4.16725, saving model to models/model-06.hdf5\n",
      "Epoch:  6  - lr: 0.0067166667  - batch: 5  - epoch:  6\n",
      "Epoch 7/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 3.5055 - policy_loss: 3.5090 - value_loss: 0.0669 - policy_acc: 0.3488 - policy_acc_reg: 1.0000 - value_acc: 0.9449 - value_acc_reg: 0.9331 - val_loss: 4.1456 - val_policy_loss: 4.1497 - val_value_loss: 0.0455 - val_policy_acc: 0.2883 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9842 - val_value_acc_reg: 0.9545y_acc_reg: 1.0000 - value_acc: 0.942 - ETA: 22s - loss: 3.5436 - policy_loss: 3.5471 - value_loss: 0.0685 - policy_acc: 0.3440 - policy_acc_reg: 1.0000 - value_acc: 0.9411 - valu - ETA: 19s - loss: 3.5359 - policy_loss: 3.5394 - value_loss: 0.0687 - policy_acc: 0.3445 - ETA: 8s - loss: 3.4955 - policy_loss: 3.4989 - value_loss: 0.0674 - p\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.16725 to 4.14564, saving model to models/model-07.hdf5\n",
      "Epoch:  7  - lr: 0.0051416666  - batch: 6  - epoch:  7\n",
      "Epoch 8/20\n",
      "351/351 [==============================] - 70s 198ms/step - loss: 3.1543 - policy_loss: 3.1574 - value_loss: 0.0663 - policy_acc: 0.3917 - policy_acc_reg: 1.0000 - value_acc: 0.9507 - value_acc_reg: 0.9337 - val_loss: 3.5482 - val_policy_loss: 3.5517 - val_value_loss: 0.0537 - val_policy_acc: 0.3389 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9708 - val_value_acc_reg: 0.9463: 3.1433 - value_loss: 0.0661 - pol\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.14564 to 3.54817, saving model to models/model-08.hdf5\n",
      "Epoch:  8  - lr: 0.0035666667  - batch: 7  - epoch:  8\n",
      "Epoch 9/20\n",
      "351/351 [==============================] - 69s 198ms/step - loss: 2.8649 - policy_loss: 2.8677 - value_loss: 0.0647 - policy_acc: 0.4293 - policy_acc_reg: 1.0000 - value_acc: 0.9545 - value_acc_reg: 0.9353 - val_loss: 3.4172 - val_policy_loss: 3.4205 - val_value_loss: 0.0476 - val_policy_acc: 0.3569 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9800 - val_value_acc_reg: 0.9524 policy_acc: 0.4256 - policy_acc_reg: 1.0000 - value_acc: 0.9539 - value_acc - ETA: 22s - loss: 2.9024 - policy_loss: 2.9052 - value_loss: 0.0646 - policy_acc: 0.4258 - policy_acc_reg: 1.0000 - va - ETA: 15s - loss: 2.8795 - policy_loss: 2.8823 - value_loss: 0.0645 - policy_acc: 0.4283 - policy_acc_reg: 1.0000 - value_acc: 0.9543 - value_ - ETA: 12s - loss: 2.8732 - policy_loss: \n",
      "\n",
      "Epoch 00009: val_loss improved from 3.54817 to 3.41716, saving model to models/model-09.hdf5\n",
      "Epoch:  9  - lr: 0.0019916666  - batch: 8  - epoch:  9\n",
      "Epoch 10/20\n",
      "351/351 [==============================] - 69s 198ms/step - loss: 2.5790 - policy_loss: 2.5815 - value_loss: 0.0626 - policy_acc: 0.4747 - policy_acc_reg: 1.0000 - value_acc: 0.9577 - value_acc_reg: 0.9374 - val_loss: 3.2624 - val_policy_loss: 3.2656 - val_value_loss: 0.0542 - val_policy_acc: 0.3775 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9719 - val_value_acc_reg: 0.9458olicy_acc: 0.4787 - - ETA: 16s - loss: 2.6206 - policy_loss: 2.6232 - value_loss: 0.0631 - policy_acc: 0.4690 - policy_acc_reg: 1.0000 - value_acc: 0.9566 - value_acc_reg: 0 - ETA: 15s - loss: 2.61\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.41716 to 3.26240, saving model to models/model-10.hdf5\n",
      "Epoch:  10  - lr: 0.000975  - batch: 9  - epoch:  10\n",
      "Epoch 11/20\n",
      "351/351 [==============================] - 70s 198ms/step - loss: 2.4086 - policy_loss: 2.4109 - value_loss: 0.0604 - policy_acc: 0.5048 - policy_acc_reg: 1.0000 - value_acc: 0.9599 - value_acc_reg: 0.9396 - val_loss: 3.2442 - val_policy_loss: 3.2474 - val_value_loss: 0.0549 - val_policy_acc: 0.3802 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9651 - val_value_acc_reg: 0.9451\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.26240 to 3.24423, saving model to models/model-11.hdf5\n",
      "Epoch:  11  - lr: 0.0009075  - batch: 10  - epoch:  11\n",
      "Epoch 12/20\n",
      "351/351 [==============================] - 69s 198ms/step - loss: 2.2918 - policy_loss: 2.2940 - value_loss: 0.0628 - policy_acc: 0.5235 - policy_acc_reg: 1.0000 - value_acc: 0.9559 - value_acc_reg: 0.9372 - val_loss: 3.2569 - val_policy_loss: 3.2601 - val_value_loss: 0.0573 - val_policy_acc: 0.3732 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9657 - val_value_acc_reg: 0.9427 - policy_loss: 2.1819 - value_loss: 0.0639 - policy_acc: 0.54 - ETA: 49s - loss: 2.2532 - policy_loss: 2.2554 - value_loss: 0.0627 - policy_acc: 0.5328 - policy_acc_reg: 1.0000 - value_acc: 0.9560 - ETA: 44s - loss: 2.2614 - policy_loss: 2.2636 - value_loss: 0.0628 - policy_acc: 0.5317 - policy_acc_reg: 1.0000 - value_acc: 0.9557 - value_acc_r - ETA: 42s - loss:  - ETA: 16s - loss: \n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.24423\n",
      "Epoch:  12  - lr: 0.00084  - batch: 11  - epoch:  12\n",
      "Epoch 13/20\n",
      "351/351 [==============================] - 69s 197ms/step - loss: 2.1704 - policy_loss: 2.1725 - value_loss: 0.0621 - policy_acc: 0.5466 - policy_acc_reg: 1.0000 - value_acc: 0.9572 - value_acc_reg: 0.9379 - val_loss: 3.2609 - val_policy_loss: 3.2641 - val_value_loss: 0.0515 - val_policy_acc: 0.3753 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9740 - val_value_acc_reg: 0.9485loss: 0.0623 - policy_acc: 0.5515\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.24423\n",
      "Epoch:  13  - lr: 0.0007725  - batch: 12  - epoch:  13\n",
      "Epoch 14/20\n",
      "351/351 [==============================] - 69s 197ms/step - loss: 2.0520 - policy_loss: 2.0540 - value_loss: 0.0620 - policy_acc: 0.5684 - policy_acc_reg: 1.0000 - value_acc: 0.9573 - value_acc_reg: 0.9380 - val_loss: 3.2289 - val_policy_loss: 3.2321 - val_value_loss: 0.0542 - val_policy_acc: 0.3823 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9717 - val_value_acc_reg: 0.9458ic - ETA: 3s - loss: 2.0469 - policy_loss: 2.0489 - value_loss: 0.0618 - policy_acc: 0.5693 - policy_acc_reg: 1.0000 - val\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.24423 to 3.22891, saving model to models/model-14.hdf5\n",
      "Epoch:  14  - lr: 0.000705  - batch: 13  - epoch:  14\n",
      "Epoch 15/20\n",
      "351/351 [==============================] - 69s 197ms/step - loss: 1.9358 - policy_loss: 1.9377 - value_loss: 0.0616 - policy_acc: 0.5909 - policy_acc_reg: 1.0000 - value_acc: 0.9575 - value_acc_reg: 0.9384 - val_loss: 3.2513 - val_policy_loss: 3.2545 - val_value_loss: 0.0602 - val_policy_acc: 0.3796 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9622 - val_value_acc_reg: 0.9398\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.22891\n",
      "Epoch:  15  - lr: 0.0006375  - batch: 14  - epoch:  15\n",
      "Epoch 16/20\n",
      "351/351 [==============================] - 69s 197ms/step - loss: 1.8189 - policy_loss: 1.8207 - value_loss: 0.0617 - policy_acc: 0.6156 - policy_acc_reg: 1.0000 - value_acc: 0.9572 - value_acc_reg: 0.9383 - val_loss: 3.2787 - val_policy_loss: 3.2819 - val_value_loss: 0.0517 - val_policy_acc: 0.3749 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9737 - val_value_acc_reg: 0.9483\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.22891\n",
      "Epoch:  16  - lr: 0.00057  - batch: 15  - epoch:  16\n",
      "Epoch 17/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 1.6996 - policy_loss: 1.7012 - value_loss: 0.0620 - policy_acc: 0.6404 - policy_acc_reg: 1.0000 - value_acc: 0.9572 - value_acc_reg: 0.9380 - val_loss: 3.2517 - val_policy_loss: 3.2549 - val_value_loss: 0.0542 - val_policy_acc: 0.3787 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9712 - val_value_acc_reg: 0.9458\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.22891\n",
      "Epoch:  17  - lr: 0.0005025  - batch: 16  - epoch:  17\n",
      "Epoch 18/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 1.5894 - policy_loss: 1.5909 - value_loss: 0.0614 - policy_acc: 0.6666 - policy_acc_reg: 1.0000 - value_acc: 0.9577 - value_acc_reg: 0.9386 - val_loss: 3.2631 - val_policy_loss: 3.2663 - val_value_loss: 0.0645 - val_policy_acc: 0.3889 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9510 - val_value_acc_reg: 0.9355\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.22891\n",
      "Epoch:  18  - lr: 0.000435  - batch: 17  - epoch:  18\n",
      "Epoch 19/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 1.4904 - policy_loss: 1.4919 - value_loss: 0.0617 - policy_acc: 0.6886 - policy_acc_reg: 1.0000 - value_acc: 0.9564 - value_acc_reg: 0.9383 - val_loss: 3.3156 - val_policy_loss: 3.3189 - val_value_loss: 0.0586 - val_policy_acc: 0.3820 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9666 - val_value_acc_reg: 0.94142 - policy_loss: 1.4686 - value_loss: 0.0628 - policy_acc: 0.6942 - policy_ - ETA: 17s - loss: 1.4710 - policy_loss: 1.4724 - value_loss: 0.0625 - policy_acc: 0.6933 - policy_acc - ETA: 8s - loss: 1.4746 - policy_loss: 1.4760 - value_loss: 0.0615 - policy_acc: 0.6927 - policy_acc_reg: 1.0000 - val - ETA: 4s - loss: 1.4842 - policy_loss: 1.4856 - value_loss: 0.0616 - policy_acc: 0.6905 - policy_acc_reg\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.22891\n",
      "Epoch:  19  - lr: 0.0003675  - batch: 18  - epoch:  19\n",
      "Epoch 20/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 1.3983 - policy_loss: 1.3996 - value_loss: 0.0616 - policy_acc: 0.7108 - policy_acc_reg: 1.0000 - value_acc: 0.9564 - value_acc_reg: 0.9384 - val_loss: 3.2648 - val_policy_loss: 3.2680 - val_value_loss: 0.0555 - val_policy_acc: 0.3778 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9669 - val_value_acc_reg: 0.9445\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.22891\n",
      "Epoch:  20  - lr: 0.0003  - batch: 19  - epoch:  20\n"
     ]
    }
   ],
   "source": [
    "# model.save('models\\\\BughouseNet220620190437.h5')\n",
    "history = model.fit_generator(generator(batch_size,len(train_ids),path=dataset_path), steps_per_epoch=int((len(train_ids)*zip_length)/batch_size), callbacks=callbacks_list,\n",
    "                    epochs=epochs, validation_data=(x_val, [policies_val,values_val]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  3/351 [..............................] - ETA: 6:02 - loss: 7.9959 - policy_loss: 8.0039 - value_loss: 0.0590 - policy_acc: 0.0013 - policy_acc_reg: 1.0000 - value_acc: 0.9557 - value_acc_reg: 0.9410   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoBray\\.conda\\envs\\TensorFlow_GPU_Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.273766). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 71s 202ms/step - loss: 6.3648 - policy_loss: 6.3711 - value_loss: 0.0418 - policy_acc: 0.0640 - policy_acc_reg: 1.0000 - value_acc: 0.9800 - value_acc_reg: 0.9582 - val_loss: 5.7299 - val_policy_loss: 5.7355 - val_value_loss: 0.0834 - val_policy_acc: 0.0719 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9045 - val_value_acc_reg: 0.9166: 4s - loss: 6.4047 - policy_loss: 6.4111 - value_loss: 0.0422 - policy_acc: 0.0627 - policy_acc_reg: 1.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.72986, saving model to models/model-01.hdf5\n",
      "Epoch:  1  - lr: 0.0014083333  - batch: 0  - epoch:  1\n",
      "Epoch 2/20\n",
      "351/351 [==============================] - 68s 195ms/step - loss: 5.4874 - policy_loss: 5.4929 - value_loss: 0.0426 - policy_acc: 0.1075 - policy_acc_reg: 1.0000 - value_acc: 0.9803 - value_acc_reg: 0.9574 - val_loss: 5.8118 - val_policy_loss: 5.8176 - val_value_loss: 0.0696 - val_policy_acc: 0.0746 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9587 - val_value_acc_reg: 0.9304200 - policy_loss: 5.5255 - value_loss: 0.0444 - policy_acc: 0.1029 - policy_acc_reg: 1.0000 - value_acc: 0.97 - ETA: 7s - loss: 5.5095 - policy_loss: 5.5150 - value_loss: 0.0437 - policy_acc: 0.1044 - policy_acc_reg: 1.0000 - value_acc: 0.9793 - val - ETA: 5s - loss: 5.5058 - policy_loss: 5.5113 - value_loss: 0.0433 - policy_acc: 0.1047 - po\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 5.72986\n",
      "Epoch:  2  - lr: 0.0029833333  - batch: 1  - epoch:  2\n",
      "Epoch 3/20\n",
      "351/351 [==============================] - 69s 195ms/step - loss: 5.1726 - policy_loss: 5.1777 - value_loss: 0.0475 - policy_acc: 0.1520 - policy_acc_reg: 1.0000 - value_acc: 0.9690 - value_acc_reg: 0.9525 - val_loss: 5.7314 - val_policy_loss: 5.7370 - val_value_loss: 0.1005 - val_policy_acc: 0.1126 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.8856 - val_value_acc_reg: 0.8995icy_acc: 0.1391 - policy - ETA: 16s - loss: 5.1704 - policy_loss: 5.1755 - value_loss: 0.0467 - policy_acc: 0.1503 - policy_acc_reg: 1.0000 - value_ - ETA: 9s - loss: 5.1706 - policy_loss: 5.1757 - val\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.72986\n",
      "Epoch:  3  - lr: 0.004558333  - batch: 2  - epoch:  3\n",
      "Epoch 4/20\n",
      "351/351 [==============================] - 68s 195ms/step - loss: 4.8312 - policy_loss: 4.8360 - value_loss: 0.0535 - policy_acc: 0.1985 - policy_acc_reg: 1.0000 - value_acc: 0.9600 - value_acc_reg: 0.9465 - val_loss: 5.7150 - val_policy_loss: 5.7207 - val_value_loss: 0.0534 - val_policy_acc: 0.1049 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9737 - val_value_acc_reg: 0.9466\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.72986 to 5.71505, saving model to models/model-04.hdf5\n",
      "Epoch:  4  - lr: 0.0061333333  - batch: 3  - epoch:  4\n",
      "Epoch 5/20\n",
      "351/351 [==============================] - 68s 195ms/step - loss: 4.3592 - policy_loss: 4.3635 - value_loss: 0.0510 - policy_acc: 0.2578 - policy_acc_reg: 1.0000 - value_acc: 0.9663 - value_acc_reg: 0.9490 - val_loss: 4.6205 - val_policy_loss: 4.6250 - val_value_loss: 0.1010 - val_policy_acc: 0.2224 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.8851 - val_value_acc_reg: 0.8990\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.71505 to 4.62049, saving model to models/model-05.hdf5\n",
      "Epoch:  5  - lr: 0.0077083334  - batch: 4  - epoch:  5\n",
      "Epoch 6/20\n",
      "351/351 [==============================] - 69s 195ms/step - loss: 3.9449 - policy_loss: 3.9488 - value_loss: 0.0526 - policy_acc: 0.3009 - policy_acc_reg: 1.0000 - value_acc: 0.9663 - value_acc_reg: 0.9474 - val_loss: 4.1172 - val_policy_loss: 4.1213 - val_value_loss: 0.0449 - val_policy_acc: 0.2760 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9727 - val_value_acc_reg: 0.95510.0520 - policy_acc: 0 - ETA: 15s - loss: 3.9534 - policy_loss: 3.9573 - value_loss: 0.0531 - policy_acc: 0.3000 - policy_acc_reg: - ETA: 7s - loss: 3.9405 - policy_loss: 3.9444 - value_loss: 0.0528 - polic\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.62049 to 4.11720, saving model to models/model-06.hdf5\n",
      "Epoch:  6  - lr: 0.0067166667  - batch: 5  - epoch:  6\n",
      "Epoch 7/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 3.5661 - policy_loss: 3.5696 - value_loss: 0.0528 - policy_acc: 0.3400 - policy_acc_reg: 1.0000 - value_acc: 0.9629 - value_acc_reg: 0.9472 - val_loss: 3.7871 - val_policy_loss: 3.7908 - val_value_loss: 0.0482 - val_policy_acc: 0.3106 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9800 - val_value_acc_reg: 0.9518ss: 0.0532 - policy_acc: 0.3384 - policy_acc_reg: 1.0000 - value_ - ETA: 16s - loss: 3.5794 - policy_loss: 3.5829 - value_loss: 0.0537 - policy_acc: 0.3387 - policy_acc_reg: 1.0000 - value_acc: 0.9601 - value_ - ETA: 13s - loss: 3.5847 - poli\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.11720 to 3.78707, saving model to models/model-07.hdf5\n",
      "Epoch:  7  - lr: 0.0051416666  - batch: 6  - epoch:  7\n",
      "Epoch 8/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 3.2437 - policy_loss: 3.2469 - value_loss: 0.0504 - policy_acc: 0.3789 - policy_acc_reg: 1.0000 - value_acc: 0.9698 - value_acc_reg: 0.9496 - val_loss: 3.5931 - val_policy_loss: 3.5967 - val_value_loss: 0.0448 - val_policy_acc: 0.3349 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9826 - val_value_acc_reg: 0.95522744 - value_loss: 0.0508 - policy_acc: 0.3731 - policy_acc_reg: 1.0000 -  - ETA: 16s - loss: 3.2574 - policy_loss: 3.2606 - value_loss: 0.0508 - policy_acc: 0.3755 - policy_acc_reg: 1 - ETA: 8s - loss: 3.2424 - policy_loss: 3.2456 - value_loss: 0.0505 - policy_acc: 0.3784 - policy_acc_reg: 1.0000 - value_acc - ETA: 5s - loss: 3.2406 - policy_loss: 3.2438 - value_loss: 0.0504 - policy_acc: 0.3788 - policy\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.78707 to 3.59313, saving model to models/model-08.hdf5\n",
      "Epoch:  8  - lr: 0.0035666667  - batch: 7  - epoch:  8\n",
      "Epoch 9/20\n",
      "351/351 [==============================] - 68s 195ms/step - loss: 2.9481 - policy_loss: 2.9510 - value_loss: 0.0493 - policy_acc: 0.4177 - policy_acc_reg: 1.0000 - value_acc: 0.9732 - value_acc_reg: 0.9507 - val_loss: 3.4958 - val_policy_loss: 3.4992 - val_value_loss: 0.0470 - val_policy_acc: 0.3479 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9760 - val_value_acc_reg: 0.9530\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.59313 to 3.49578, saving model to models/model-09.hdf5\n",
      "Epoch:  9  - lr: 0.0019916666  - batch: 8  - epoch:  9\n",
      "Epoch 10/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 2.6883 - policy_loss: 2.6909 - value_loss: 0.0516 - policy_acc: 0.4579 - policy_acc_reg: 1.0000 - value_acc: 0.9701 - value_acc_reg: 0.9484 - val_loss: 3.3064 - val_policy_loss: 3.3097 - val_value_loss: 0.0476 - val_policy_acc: 0.3684 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9756 - val_value_acc_reg: 0.9524- policy_acc: 0.4514 - policy_acc_reg: 1.0000 - value_acc: 0.9686 - value_acc_reg:  - ETA: 34s - loss: 2.7312 - policy_loss: 2.7338 - value_loss: 0.0526 - po - ETA: 18s - loss: 2.7009 - policy_loss: - ETA: 3s - loss: 2.6948 - policy_loss: 2.6974 - value_loss: 0.0519 - policy_acc: 0.4573 - policy_acc_reg: 1.0000 - value_a\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.49578 to 3.30640, saving model to models/model-10.hdf5\n",
      "Epoch:  10  - lr: 0.000975  - batch: 9  - epoch:  10\n",
      "Epoch 11/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 2.5310 - policy_loss: 2.5335 - value_loss: 0.0497 - policy_acc: 0.4843 - policy_acc_reg: 1.0000 - value_acc: 0.9725 - value_acc_reg: 0.9503 - val_loss: 3.2634 - val_policy_loss: 3.2667 - val_value_loss: 0.0475 - val_policy_acc: 0.3746 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9757 - val_value_acc_reg: 0.9525- value_acc_r\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.30640 to 3.26344, saving model to models/model-11.hdf5\n",
      "Epoch:  11  - lr: 0.0009075  - batch: 10  - epoch:  11\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 68s 193ms/step - loss: 2.4175 - policy_loss: 2.4199 - value_loss: 0.0504 - policy_acc: 0.5039 - policy_acc_reg: 1.0000 - value_acc: 0.9717 - value_acc_reg: 0.9496 - val_loss: 3.2652 - val_policy_loss: 3.2684 - val_value_loss: 0.0492 - val_policy_acc: 0.3718 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9718 - val_value_acc_reg: 0.9508\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.26344\n",
      "Epoch:  12  - lr: 0.00084  - batch: 11  - epoch:  12\n",
      "Epoch 13/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 2.3114 - policy_loss: 2.3137 - value_loss: 0.0498 - policy_acc: 0.5231 - policy_acc_reg: 1.0000 - value_acc: 0.9723 - value_acc_reg: 0.9502 - val_loss: 3.2726 - val_policy_loss: 3.2759 - val_value_loss: 0.0485 - val_policy_acc: 0.3723 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9759 - val_value_acc_reg: 0.9515\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.26344\n",
      "Epoch:  13  - lr: 0.0007725  - batch: 12  - epoch:  13\n",
      "Epoch 14/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 2.1992 - policy_loss: 2.2014 - value_loss: 0.0501 - policy_acc: 0.5423 - policy_acc_reg: 1.0000 - value_acc: 0.9731 - value_acc_reg: 0.9499 - val_loss: 3.2680 - val_policy_loss: 3.2712 - val_value_loss: 0.0505 - val_policy_acc: 0.3718 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9713 - val_value_acc_reg: 0.9495loss: 2.2046 - value_loss: 0.0496 - polic - ETA: 5s - loss: 2.2004 - policy_loss: 2.2026 - value_loss: 0.0500 - policy_acc: 0.5420 - policy_acc_r\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.26344\n",
      "Epoch:  14  - lr: 0.000705  - batch: 13  - epoch:  14\n",
      "Epoch 15/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 2.0925 - policy_loss: 2.0945 - value_loss: 0.0502 - policy_acc: 0.5641 - policy_acc_reg: 1.0000 - value_acc: 0.9719 - value_acc_reg: 0.9498 - val_loss: 3.2468 - val_policy_loss: 3.2500 - val_value_loss: 0.0431 - val_policy_acc: 0.3756 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9810 - val_value_acc_reg: 0.9569ss: 2.0944 - value_loss: 0.0502 - policy_acc: 0.5642 - policy_acc_reg: 1.0000 - value_acc: 0.9719 - value_acc_reg: 0.94\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.26344 to 3.24676, saving model to models/model-15.hdf5\n",
      "Epoch:  15  - lr: 0.0006375  - batch: 14  - epoch:  15\n",
      "Epoch 16/20\n",
      "351/351 [==============================] - 70s 199ms/step - loss: 1.9748 - policy_loss: 1.9767 - value_loss: 0.0512 - policy_acc: 0.5863 - policy_acc_reg: 1.0000 - value_acc: 0.9701 - value_acc_reg: 0.9488 - val_loss: 3.2554 - val_policy_loss: 3.2586 - val_value_loss: 0.0464 - val_policy_acc: 0.3741 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9774 - val_value_acc_reg: 0.95361.9650 - value_loss: 0.0515 - policy_acc: 0.5892 - policy_acc_reg: 1.0000 - value_acc: 0.9699 - \n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.24676\n",
      "Epoch:  16  - lr: 0.00057  - batch: 15  - epoch:  16\n",
      "Epoch 17/20\n",
      "351/351 [==============================] - 86s 246ms/step - loss: 1.8695 - policy_loss: 1.8713 - value_loss: 0.0502 - policy_acc: 0.6105 - policy_acc_reg: 1.0000 - value_acc: 0.9717 - value_acc_reg: 0.9498 - val_loss: 3.2438 - val_policy_loss: 3.2470 - val_value_loss: 0.0445 - val_policy_acc: 0.3809 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9786 - val_value_acc_reg: 0.9555\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.24676 to 3.24379, saving model to models/model-17.hdf5\n",
      "Epoch:  17  - lr: 0.0005025  - batch: 16  - epoch:  17\n",
      "Epoch 18/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 1.7639 - policy_loss: 1.7656 - value_loss: 0.0503 - policy_acc: 0.6317 - policy_acc_reg: 1.0000 - value_acc: 0.9714 - value_acc_reg: 0.9497 - val_loss: 3.2418 - val_policy_loss: 3.2450 - val_value_loss: 0.0502 - val_policy_acc: 0.3795 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9741 - val_value_acc_reg: 0.9498\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.24379 to 3.24185, saving model to models/model-18.hdf5\n",
      "Epoch:  18  - lr: 0.000435  - batch: 17  - epoch:  18\n",
      "Epoch 19/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 1.6736 - policy_loss: 1.6752 - value_loss: 0.0501 - policy_acc: 0.6505 - policy_acc_reg: 1.0000 - value_acc: 0.9726 - value_acc_reg: 0.9499 - val_loss: 3.2542 - val_policy_loss: 3.2574 - val_value_loss: 0.0473 - val_policy_acc: 0.3821 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9792 - val_value_acc_reg: 0.95270000 - value_acc: 0.9740 - value_acc_ - ETA: 40s - loss: 1.6633 - policy_loss: 1.6650 - value_loss: 0.0495 - policy_acc: 0.6556 - policy_acc_reg: 1.0000 - value_acc: 0.9735 - value_a - ETA: 37s - loss: 1.6641 - policy_loss: 1.6657 - value_loss: 0.0493 - policy_acc: 0.6548 - policy_acc_reg: 1.0000 - - ETA: 29s - loss: 1.6678 - policy_loss: 1.6694 - value_loss: 0.0491 - policy_acc: 0.6526 - policy_acc_reg\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.24185\n",
      "Epoch:  19  - lr: 0.0003675  - batch: 18  - epoch:  19\n",
      "Epoch 20/20\n",
      "351/351 [==============================] - 69s 196ms/step - loss: 1.5866 - policy_loss: 1.5881 - value_loss: 0.0499 - policy_acc: 0.6721 - policy_acc_reg: 1.0000 - value_acc: 0.9732 - value_acc_reg: 0.9501 - val_loss: 3.2695 - val_policy_loss: 3.2727 - val_value_loss: 0.0491 - val_policy_acc: 0.3817 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9742 - val_value_acc_reg: 0.95099 - value_loss: 0.0501 - policy_acc: 0.6732 - policy_acc_reg: 1.0000 - value_acc: 0.9727 - value_acc_reg: 0.949 - ETA: 29s - loss: 1.5896 - policy_loss: 1.5912 - value_loss: 0.0501 - policy_acc: 0.6730 - policy_acc_reg: 1\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.24185\n",
      "Epoch:  20  - lr: 0.0003  - batch: 19  - epoch:  20\n"
     ]
    }
   ],
   "source": [
    "# model.save('models\\\\BughouseNet220620190437.h5')\n",
    "history = model.fit_generator(generator(batch_size,len(train_ids),path=dataset_path), steps_per_epoch=int((len(train_ids)*zip_length)/batch_size), callbacks=callbacks_list,\n",
    "                    epochs=epochs, validation_data=(x_val, [policies_val,values_val]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  3/351 [..............................] - ETA: 5:37 - loss: 7.8784 - policy_loss: 7.8863 - value_loss: 0.0273 - policy_acc: 0.0000e+00 - policy_acc_reg: 1.0000 - value_acc: 1.0000 - value_acc_reg: 0.9727"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoBray\\.conda\\envs\\TensorFlow_GPU_Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.189484). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 70s 199ms/step - loss: 6.5544 - policy_loss: 6.5609 - value_loss: 0.0306 - policy_acc: 0.0601 - policy_acc_reg: 1.0000 - value_acc: 0.9980 - value_acc_reg: 0.9694 - val_loss: 6.0043 - val_policy_loss: 6.0103 - val_value_loss: 0.0417 - val_policy_acc: 0.0816 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9831 - val_value_acc_reg: 0.9583\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.00434, saving model to models/model-01.hdf5\n",
      "Epoch:  1  - lr: 0.0014083333  - batch: 0  - epoch:  1\n",
      "Epoch 2/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 5.6100 - policy_loss: 5.6155 - value_loss: 0.0323 - policy_acc: 0.1052 - policy_acc_reg: 1.0000 - value_acc: 0.9960 - value_acc_reg: 0.9677 - val_loss: 5.6622 - val_policy_loss: 5.6678 - val_value_loss: 0.0477 - val_policy_acc: 0.0789 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9824 - val_value_acc_reg: 0.9523 policy_acc: 0.0766 - policy_acc_reg: 1.0000 - va - ETA: 51s - loss: 5.8195 - policy_loss: 5.8253 - value_loss: 0.0304 - policy_acc: 0.0855 - policy_acc_reg: 1.0000 -  - ETA: 43s - loss: 5.7763 -\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.00434 to 5.66221, saving model to models/model-02.hdf5\n",
      "Epoch:  2  - lr: 0.0029833333  - batch: 1  - epoch:  2\n",
      "Epoch 3/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 5.2234 - policy_loss: 5.2285 - value_loss: 0.0462 - policy_acc: 0.1501 - policy_acc_reg: 1.0000 - value_acc: 0.9816 - value_acc_reg: 0.9538 - val_loss: 5.8453 - val_policy_loss: 5.8511 - val_value_loss: 0.0194 - val_policy_acc: 0.1111 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9999 - val_value_acc_reg: 0.9806- policy_loss: 5.3551 - value_loss: 0.0377 - policy_acc: 0.1348 - policy_acc_reg: 1.0000 - value_acc: 0.9945 - value_acc_reg: 0 - ETA: 57s - loss: 5.3381 - policy_loss: 5.3434 - value_loss: 0.0395 - polic - ETA: 42s - loss: 5.2616 - policy_loss: 5.2668 - value_loss: 0.0386 - policy_acc: 0.1448 - policy_acc_reg: 1.0000 - value_acc: 0.990 - ETA: 37s - loss: 5.2812 - policy_loss: 5.2864 - value_loss: 0.0391 - policy_acc: 0.1439 - policy_acc_reg: 1.0000 - value_acc: 0.9897 - value_acc - ETA: 35s - loss: 5.2839 - policy_loss: 5.2892 - value_loss: 0.0382 - policy_acc: 0.1434 - policy_acc_ - ETA: 25s - loss: 5.2710 - policy_loss: 5.2762 - value_loss: 0.0426 - policy_acc: 0.1433 - po - ETA: 13s - loss: 5.2353 - policy\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.66221\n",
      "Epoch:  3  - lr: 0.004558333  - batch: 2  - epoch:  3\n",
      "Epoch 4/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 4.9066 - policy_loss: 4.9115 - value_loss: 0.0367 - policy_acc: 0.1932 - policy_acc_reg: 1.0000 - value_acc: 0.9911 - value_acc_reg: 0.9633 - val_loss: 4.8882 - val_policy_loss: 4.8930 - val_value_loss: 0.0894 - val_policy_acc: 0.2032 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9057 - val_value_acc_reg: 0.9106\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.66221 to 4.88824, saving model to models/model-04.hdf5\n",
      "Epoch:  4  - lr: 0.0061333333  - batch: 3  - epoch:  4\n",
      "Epoch 5/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 4.4654 - policy_loss: 4.4698 - value_loss: 0.0462 - policy_acc: 0.2505 - policy_acc_reg: 1.0000 - value_acc: 0.9819 - value_acc_reg: 0.9538 - val_loss: 4.6728 - val_policy_loss: 4.6774 - val_value_loss: 0.0560 - val_policy_acc: 0.2228 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9764 - val_value_acc_reg: 0.9440oss: 4.4809 - policy_loss: 4.4853 - value_loss: 0.0450 - policy_acc: 0.2477 - pol - ETA: 17s - loss: 4.4688 - policy_loss: 4.4732 - value_loss: 0.0460 - policy_acc: 0.2489 - policy_acc_reg: 1.0000 -  - ETA: 9s - loss: 4.4779 - policy_loss: 4.4823 - valu\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.88824 to 4.67279, saving model to models/model-05.hdf5\n",
      "Epoch:  5  - lr: 0.0077083334  - batch: 4  - epoch:  5\n",
      "Epoch 6/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 4.0294 - policy_loss: 4.0334 - value_loss: 0.0418 - policy_acc: 0.2986 - policy_acc_reg: 1.0000 - value_acc: 0.9850 - value_acc_reg: 0.9582 - val_loss: 4.4019 - val_policy_loss: 4.4063 - val_value_loss: 0.0431 - val_policy_acc: 0.2452 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9893 - val_value_acc_reg: 0.9569y_loss: 4.0784 - value_loss: 0.0447 - policy_acc: 0.2982 - pol - ETA: 43s - loss: 4.1192 - policy_loss: 4.1233 - value_loss: 0.0451 - policy_acc: 0.2890 - policy_acc_reg: 1.0000 - value_acc: 0.9820 - value_acc_reg: 0.95 - ETA: 43s - loss: 4.1229 - policy_loss: 4.1270 - value_loss: 0.0449 - policy_acc: 0.2888 - policy_acc_reg: 1.0000 - value_acc: 0.9821  - ETA: 38s - loss: 4.1198 - policy_loss: 4.1239 - value_loss: 0.0440 - policy_acc: 0.2879 - policy_acc_reg: 1.0000 - value_acc: 0.9829 - - ETA: 34s - loss: 4.1163 - policy_loss: 4.1203 - valu - ETA: 15s - loss: 4.0578 - policy_loss: 4.0619 - value_loss: 0.0431 - policy_acc: 0.2946 - policy_acc_reg: 1.0000 - value_acc: 0.9839 - value_acc_r - ETA: 13s - loss: 4.0601 - policy_loss: 4.0641 - value_loss: 0.0430 - policy_acc: 0.2945 - policy_acc_reg: 1 - ETA: 7s - loss: 4.0583 - policy_loss: 4.0623 - value_loss: 0.0423 - policy_acc:\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.67279 to 4.40192, saving model to models/model-06.hdf5\n",
      "Epoch:  6  - lr: 0.0067166667  - batch: 5  - epoch:  6\n",
      "Epoch 7/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 3.6615 - policy_loss: 3.6652 - value_loss: 0.0414 - policy_acc: 0.3379 - policy_acc_reg: 1.0000 - value_acc: 0.9849 - value_acc_reg: 0.9586 - val_loss: 3.9388 - val_policy_loss: 3.9427 - val_value_loss: 0.0568 - val_policy_acc: 0.2968 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9751 - val_value_acc_reg: 0.9432 - loss: 3.6586 - policy_\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.40192 to 3.93885, saving model to models/model-07.hdf5\n",
      "Epoch:  7  - lr: 0.0051416666  - batch: 6  - epoch:  7\n",
      "Epoch 8/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 3.3526 - policy_loss: 3.3560 - value_loss: 0.0423 - policy_acc: 0.3745 - policy_acc_reg: 1.0000 - value_acc: 0.9831 - value_acc_reg: 0.9577 - val_loss: 3.8309 - val_policy_loss: 3.8347 - val_value_loss: 0.0405 - val_policy_acc: 0.3101 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9899 - val_value_acc_reg: 0.95953.3676 - value_loss: 0.0420 - policy_acc: 0.3730 - policy_acc_reg: 1.0000 - value_acc - ETA: 16s - los\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.93885 to 3.83089, saving model to models/model-08.hdf5\n",
      "Epoch:  8  - lr: 0.0035666667  - batch: 7  - epoch:  8\n",
      "Epoch 9/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 3.0611 - policy_loss: 3.0641 - value_loss: 0.0410 - policy_acc: 0.4113 - policy_acc_reg: 1.0000 - value_acc: 0.9852 - value_acc_reg: 0.9590 - val_loss: 3.5870 - val_policy_loss: 3.5906 - val_value_loss: 0.0466 - val_policy_acc: 0.3506 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9823 - val_value_acc_reg: 0.9534\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.83089 to 3.58703, saving model to models/model-09.hdf5\n",
      "Epoch:  9  - lr: 0.0019916666  - batch: 8  - epoch:  9\n",
      "Epoch 10/20\n",
      "351/351 [==============================] - 67s 192ms/step - loss: 2.8151 - policy_loss: 2.8179 - value_loss: 0.0417 - policy_acc: 0.4466 - policy_acc_reg: 1.0000 - value_acc: 0.9828 - value_acc_reg: 0.9583 - val_loss: 3.3381 - val_policy_loss: 3.3414 - val_value_loss: 0.0392 - val_policy_acc: 0.3690 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9860 - val_value_acc_reg: 0.9608\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.58703 to 3.33810, saving model to models/model-10.hdf5\n",
      "Epoch:  10  - lr: 0.000975  - batch: 9  - epoch:  10\n",
      "Epoch 11/20\n",
      "351/351 [==============================] - 67s 191ms/step - loss: 2.6483 - policy_loss: 2.6510 - value_loss: 0.0432 - policy_acc: 0.4748 - policy_acc_reg: 1.0000 - value_acc: 0.9816 - value_acc_reg: 0.9568 - val_loss: 3.3561 - val_policy_loss: 3.3594 - val_value_loss: 0.0435 - val_policy_acc: 0.3639 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9842 - val_value_acc_reg: 0.9565\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.33810\n",
      "Epoch:  11  - lr: 0.0009075  - batch: 10  - epoch:  11\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 68s 194ms/step - loss: 2.5409 - policy_loss: 2.5434 - value_loss: 0.0438 - policy_acc: 0.4907 - policy_acc_reg: 1.0000 - value_acc: 0.9818 - value_acc_reg: 0.9562 - val_loss: 3.3085 - val_policy_loss: 3.3118 - val_value_loss: 0.0438 - val_policy_acc: 0.3679 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9819 - val_value_acc_reg: 0.9562\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.33810 to 3.30849, saving model to models/model-12.hdf5\n",
      "Epoch:  12  - lr: 0.00084  - batch: 11  - epoch:  12\n",
      "Epoch 13/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 2.4171 - policy_loss: 2.4194 - value_loss: 0.0439 - policy_acc: 0.5130 - policy_acc_reg: 1.0000 - value_acc: 0.9816 - value_acc_reg: 0.9561 - val_loss: 3.3186 - val_policy_loss: 3.3218 - val_value_loss: 0.0415 - val_policy_acc: 0.3744 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9853 - val_value_acc_reg: 0.9585\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.30849\n",
      "Epoch:  13  - lr: 0.0007725  - batch: 12  - epoch:  13\n",
      "Epoch 14/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 2.3151 - policy_loss: 2.3174 - value_loss: 0.0446 - policy_acc: 0.5302 - policy_acc_reg: 1.0000 - value_acc: 0.9786 - value_acc_reg: 0.9554 - val_loss: 3.3084 - val_policy_loss: 3.3117 - val_value_loss: 0.0411 - val_policy_acc: 0.3759 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9848 - val_value_acc_reg: 0.9589\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.30849 to 3.30844, saving model to models/model-14.hdf5\n",
      "Epoch:  14  - lr: 0.000705  - batch: 13  - epoch:  14\n",
      "Epoch 15/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 2.1919 - policy_loss: 2.1940 - value_loss: 0.0457 - policy_acc: 0.5540 - policy_acc_reg: 1.0000 - value_acc: 0.9792 - value_acc_reg: 0.9543 - val_loss: 3.3105 - val_policy_loss: 3.3137 - val_value_loss: 0.0411 - val_policy_acc: 0.3716 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9848 - val_value_acc_reg: 0.9589 - policy_loss: 2.0752 - value_loss: 0.0448 - policy_acc: 0.5776 - policy_acc_reg: 1.0000 - value_acc: 0.9816 - value_acc_reg - ETA: 48s - loss: 2.0952 - policy_loss: 2.0972 - value_loss: 0.0447 - policy_acc: 0.5732 - policy_acc_reg: 1.0000 - value_acc: 0.9814 - - ETA: 44s - loss: 2.136\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.30844\n",
      "Epoch:  15  - lr: 0.0006375  - batch: 14  - epoch:  15\n",
      "Epoch 16/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 2.0878 - policy_loss: 2.0898 - value_loss: 0.0477 - policy_acc: 0.5727 - policy_acc_reg: 1.0000 - value_acc: 0.9771 - value_acc_reg: 0.9523 - val_loss: 3.3091 - val_policy_loss: 3.3123 - val_value_loss: 0.0437 - val_policy_acc: 0.3713 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9824 - val_value_acc_reg: 0.9563\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.30844\n",
      "Epoch:  16  - lr: 0.00057  - batch: 15  - epoch:  16\n",
      "Epoch 17/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 1.9737 - policy_loss: 1.9756 - value_loss: 0.0464 - policy_acc: 0.5961 - policy_acc_reg: 1.0000 - value_acc: 0.9778 - value_acc_reg: 0.9536 - val_loss: 3.3243 - val_policy_loss: 3.3276 - val_value_loss: 0.0435 - val_policy_acc: 0.3709 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9831 - val_value_acc_reg: 0.9565\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.30844\n",
      "Epoch:  17  - lr: 0.0005025  - batch: 16  - epoch:  17\n",
      "Epoch 18/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 1.8623 - policy_loss: 1.8641 - value_loss: 0.0470 - policy_acc: 0.6170 - policy_acc_reg: 1.0000 - value_acc: 0.9771 - value_acc_reg: 0.9530 - val_loss: 3.3377 - val_policy_loss: 3.3410 - val_value_loss: 0.0499 - val_policy_acc: 0.3751 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9774 - val_value_acc_reg: 0.9501\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.30844\n",
      "Epoch:  18  - lr: 0.000435  - batch: 17  - epoch:  18\n",
      "Epoch 19/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 1.7561 - policy_loss: 1.7578 - value_loss: 0.0482 - policy_acc: 0.6410 - policy_acc_reg: 1.0000 - value_acc: 0.9766 - value_acc_reg: 0.9518 - val_loss: 3.3453 - val_policy_loss: 3.3486 - val_value_loss: 0.0419 - val_policy_acc: 0.3723 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9856 - val_value_acc_reg: 0.9581\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.30844\n",
      "Epoch:  19  - lr: 0.0003675  - batch: 18  - epoch:  19\n",
      "Epoch 20/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 1.6589 - policy_loss: 1.6605 - value_loss: 0.0496 - policy_acc: 0.6637 - policy_acc_reg: 1.0000 - value_acc: 0.9749 - value_acc_reg: 0.9504 - val_loss: 3.3525 - val_policy_loss: 3.3558 - val_value_loss: 0.0512 - val_policy_acc: 0.3739 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9723 - val_value_acc_reg: 0.9488\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.30844\n",
      "Epoch:  20  - lr: 0.0003  - batch: 19  - epoch:  20\n"
     ]
    }
   ],
   "source": [
    "# model.save('models\\\\BughouseNet220620190437.h5')\n",
    "history = model.fit_generator(generator(batch_size,len(train_ids),path=dataset_path), steps_per_epoch=int((len(train_ids)*zip_length)/batch_size), callbacks=callbacks_list,\n",
    "                    epochs=epochs, validation_data=(x_val, [policies_val,values_val]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/351 [..............................] - ETA: 28:15 - loss: 7.8068 - policy_loss: 7.8145 - value_loss: 0.1379 - policy_acc: 0.0000e+00 - policy_acc_reg: 1.0000 - value_acc: 0.7930 - value_acc_reg: 0.8621"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoBray\\.conda\\envs\\TensorFlow_GPU_Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.915983). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 81s 230ms/step - loss: 6.7491 - policy_loss: 6.7558 - value_loss: 0.0816 - policy_acc: 0.0573 - policy_acc_reg: 1.0000 - value_acc: 0.9255 - value_acc_reg: 0.9184 - val_loss: 6.0614 - val_policy_loss: 6.0674 - val_value_loss: 0.0348 - val_policy_acc: 0.0812 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9908 - val_value_acc_reg: 0.9652\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.06137, saving model to models/model-01.hdf5\n",
      "Epoch:  1  - lr: 0.0014083333  - batch: 0  - epoch:  1\n",
      "Epoch 2/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 5.7232 - policy_loss: 5.7289 - value_loss: 0.0434 - policy_acc: 0.1068 - policy_acc_reg: 1.0000 - value_acc: 0.9835 - value_acc_reg: 0.9566 - val_loss: 5.6548 - val_policy_loss: 5.6603 - val_value_loss: 0.1437 - val_policy_acc: 0.0998 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.8379 - val_value_acc_reg: 0.8563: 0.0419 - policy_acc: 0.0952 - policy - ETA: 23s - loss: 5.8369 - policy_loss: 5. - ETA: 6s - loss: 5.7516 - policy_loss: 5.7573 - value_loss: 0.0431 - policy_acc: 0.1047 - \n",
      "\n",
      "Epoch 00002: val_loss improved from 6.06137 to 5.65478, saving model to models/model-02.hdf5\n",
      "Epoch:  2  - lr: 0.0029833333  - batch: 1  - epoch:  2\n",
      "Epoch 3/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 5.2841 - policy_loss: 5.2893 - value_loss: 0.0531 - policy_acc: 0.1447 - policy_acc_reg: 1.0000 - value_acc: 0.9637 - value_acc_reg: 0.9469 - val_loss: 6.0462 - val_policy_loss: 6.0522 - val_value_loss: 0.0618 - val_policy_acc: 0.1156 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9581 - val_value_acc_reg: 0.9382\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.65478\n",
      "Epoch:  3  - lr: 0.004558333  - batch: 2  - epoch:  3\n",
      "Epoch 4/20\n",
      "351/351 [==============================] - 68s 195ms/step - loss: 4.9827 - policy_loss: 4.9876 - value_loss: 0.0585 - policy_acc: 0.1846 - policy_acc_reg: 1.0000 - value_acc: 0.9568 - value_acc_reg: 0.9415 - val_loss: 5.0885 - val_policy_loss: 5.0935 - val_value_loss: 0.0598 - val_policy_acc: 0.1882 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9673 - val_value_acc_reg: 0.9402\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.65478 to 5.08849, saving model to models/model-04.hdf5\n",
      "Epoch:  4  - lr: 0.0061333333  - batch: 3  - epoch:  4\n",
      "Epoch 5/20\n",
      "351/351 [==============================] - 70s 198ms/step - loss: 4.5845 - policy_loss: 4.5891 - value_loss: 0.0532 - policy_acc: 0.2364 - policy_acc_reg: 1.0000 - value_acc: 0.9647 - value_acc_reg: 0.9468 - val_loss: 5.1418 - val_policy_loss: 5.1469 - val_value_loss: 0.0317 - val_policy_acc: 0.1692 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9822 - val_value_acc_reg: 0.9683 policy_acc_reg: 1.0000 - value_acc: 0.9626 - v - ETA: 11s - loss: 4.5985 - policy_loss: 4.60\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 5.08849\n",
      "Epoch:  5  - lr: 0.0077083334  - batch: 4  - epoch:  5\n",
      "Epoch 6/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 4.1697 - policy_loss: 4.1738 - value_loss: 0.0559 - policy_acc: 0.2823 - policy_acc_reg: 1.0000 - value_acc: 0.9632 - value_acc_reg: 0.9441 - val_loss: 4.3819 - val_policy_loss: 4.3862 - val_value_loss: 0.0497 - val_policy_acc: 0.2492 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9682 - val_value_acc_reg: 0.9503\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.08849 to 4.38187, saving model to models/model-06.hdf5\n",
      "Epoch:  6  - lr: 0.0067166667  - batch: 5  - epoch:  6\n",
      "Epoch 7/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 3.7943 - policy_loss: 3.7980 - value_loss: 0.0448 - policy_acc: 0.3223 - policy_acc_reg: 1.0000 - value_acc: 0.9780 - value_acc_reg: 0.9552 - val_loss: 4.0707 - val_policy_loss: 4.0748 - val_value_loss: 0.0496 - val_policy_acc: 0.2831 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9800 - val_value_acc_reg: 0.9504\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.38187 to 4.07075, saving model to models/model-07.hdf5\n",
      "Epoch:  7  - lr: 0.0051416666  - batch: 6  - epoch:  7\n",
      "Epoch 8/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 3.4718 - policy_loss: 3.4753 - value_loss: 0.0445 - policy_acc: 0.3613 - policy_acc_reg: 1.0000 - value_acc: 0.9820 - value_acc_reg: 0.9555 - val_loss: 3.6948 - val_policy_loss: 3.6985 - val_value_loss: 0.0401 - val_policy_acc: 0.3260 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9907 - val_value_acc_reg: 0.9599\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.07075 to 3.69484, saving model to models/model-08.hdf5\n",
      "Epoch:  8  - lr: 0.0035666667  - batch: 7  - epoch:  8\n",
      "Epoch 9/20\n",
      "351/351 [==============================] - 67s 192ms/step - loss: 3.2125 - policy_loss: 3.2157 - value_loss: 0.0396 - policy_acc: 0.3926 - policy_acc_reg: 1.0000 - value_acc: 0.9882 - value_acc_reg: 0.9604 - val_loss: 3.5747 - val_policy_loss: 3.5783 - val_value_loss: 0.0357 - val_policy_acc: 0.3423 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9931 - val_value_acc_reg: 0.9643\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.69484 to 3.57474, saving model to models/model-09.hdf5\n",
      "Epoch:  9  - lr: 0.0019916666  - batch: 8  - epoch:  9\n",
      "Epoch 10/20\n",
      "351/351 [==============================] - 67s 192ms/step - loss: 2.9636 - policy_loss: 2.9665 - value_loss: 0.0377 - policy_acc: 0.4285 - policy_acc_reg: 1.0000 - value_acc: 0.9899 - value_acc_reg: 0.9623 - val_loss: 3.4111 - val_policy_loss: 3.4145 - val_value_loss: 0.0374 - val_policy_acc: 0.3623 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9904 - val_value_acc_reg: 0.9626\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.57474 to 3.41110, saving model to models/model-10.hdf5\n",
      "Epoch:  10  - lr: 0.000975  - batch: 9  - epoch:  10\n",
      "Epoch 11/20\n",
      "351/351 [==============================] - 67s 192ms/step - loss: 2.8086 - policy_loss: 2.8114 - value_loss: 0.0382 - policy_acc: 0.4546 - policy_acc_reg: 1.0000 - value_acc: 0.9901 - value_acc_reg: 0.9618 - val_loss: 3.3959 - val_policy_loss: 3.3992 - val_value_loss: 0.0384 - val_policy_acc: 0.3662 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9909 - val_value_acc_reg: 0.9616\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.41110 to 3.39588, saving model to models/model-11.hdf5\n",
      "Epoch:  11  - lr: 0.0009075  - batch: 10  - epoch:  11\n",
      "Epoch 12/20\n",
      "351/351 [==============================] - 67s 191ms/step - loss: 2.7027 - policy_loss: 2.7054 - value_loss: 0.0384 - policy_acc: 0.4710 - policy_acc_reg: 1.0000 - value_acc: 0.9899 - value_acc_reg: 0.9616 - val_loss: 3.4036 - val_policy_loss: 3.4070 - val_value_loss: 0.0428 - val_policy_acc: 0.3620 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9833 - val_value_acc_reg: 0.9572\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.39588\n",
      "Epoch:  12  - lr: 0.00084  - batch: 11  - epoch:  12\n",
      "Epoch 13/20\n",
      "351/351 [==============================] - 67s 191ms/step - loss: 2.5981 - policy_loss: 2.6007 - value_loss: 0.0391 - policy_acc: 0.4892 - policy_acc_reg: 1.0000 - value_acc: 0.9887 - value_acc_reg: 0.9609 - val_loss: 3.3628 - val_policy_loss: 3.3661 - val_value_loss: 0.0402 - val_policy_acc: 0.3757 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9869 - val_value_acc_reg: 0.9598\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.39588 to 3.36276, saving model to models/model-13.hdf5\n",
      "Epoch:  13  - lr: 0.0007725  - batch: 12  - epoch:  13\n",
      "Epoch 14/20\n",
      "351/351 [==============================] - 68s 193ms/step - loss: 2.4866 - policy_loss: 2.4890 - value_loss: 0.0397 - policy_acc: 0.5056 - policy_acc_reg: 1.0000 - value_acc: 0.9874 - value_acc_reg: 0.9603 - val_loss: 3.3840 - val_policy_loss: 3.3874 - val_value_loss: 0.0433 - val_policy_acc: 0.3674 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9820 - val_value_acc_reg: 0.9567\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.36276\n",
      "Epoch:  14  - lr: 0.000705  - batch: 13  - epoch:  14\n",
      "Epoch 15/20\n",
      "351/351 [==============================] - 67s 191ms/step - loss: 2.3791 - policy_loss: 2.3815 - value_loss: 0.0397 - policy_acc: 0.5265 - policy_acc_reg: 1.0000 - value_acc: 0.9882 - value_acc_reg: 0.9603 - val_loss: 3.4420 - val_policy_loss: 3.4454 - val_value_loss: 0.0444 - val_policy_acc: 0.3591 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9812 - val_value_acc_reg: 0.9556\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.36276\n",
      "Epoch:  15  - lr: 0.0006375  - batch: 14  - epoch:  15\n",
      "Epoch 16/20\n",
      "351/351 [==============================] - 67s 191ms/step - loss: 2.2551 - policy_loss: 2.2574 - value_loss: 0.0393 - policy_acc: 0.5494 - policy_acc_reg: 1.0000 - value_acc: 0.9873 - value_acc_reg: 0.9607 - val_loss: 3.3633 - val_policy_loss: 3.3666 - val_value_loss: 0.0417 - val_policy_acc: 0.3705 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9840 - val_value_acc_reg: 0.9583\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.36276\n",
      "Epoch:  16  - lr: 0.00057  - batch: 15  - epoch:  16\n",
      "Epoch 17/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 2.1472 - policy_loss: 2.1494 - value_loss: 0.0403 - policy_acc: 0.5694 - policy_acc_reg: 1.0000 - value_acc: 0.9864 - value_acc_reg: 0.9597 - val_loss: 3.4099 - val_policy_loss: 3.4132 - val_value_loss: 0.0418 - val_policy_acc: 0.3617 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9826 - val_value_acc_reg: 0.9582\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.36276\n",
      "Epoch:  17  - lr: 0.0005025  - batch: 16  - epoch:  17\n",
      "Epoch 18/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 2.0440 - policy_loss: 2.0460 - value_loss: 0.0403 - policy_acc: 0.5891 - policy_acc_reg: 1.0000 - value_acc: 0.9867 - value_acc_reg: 0.9597 - val_loss: 3.3995 - val_policy_loss: 3.4029 - val_value_loss: 0.0437 - val_policy_acc: 0.3745 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9789 - val_value_acc_reg: 0.9563\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.36276\n",
      "Epoch:  18  - lr: 0.000435  - batch: 17  - epoch:  18\n",
      "Epoch 19/20\n",
      "351/351 [==============================] - 68s 194ms/step - loss: 1.9384 - policy_loss: 1.9403 - value_loss: 0.0410 - policy_acc: 0.6119 - policy_acc_reg: 1.0000 - value_acc: 0.9859 - value_acc_reg: 0.9590 - val_loss: 3.4158 - val_policy_loss: 3.4192 - val_value_loss: 0.0462 - val_policy_acc: 0.3708 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9783 - val_value_acc_reg: 0.9538\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.36276\n",
      "Epoch:  19  - lr: 0.0003675  - batch: 18  - epoch:  19\n",
      "Epoch 20/20\n",
      "351/351 [==============================] - 69s 198ms/step - loss: 1.8337 - policy_loss: 1.8355 - value_loss: 0.0414 - policy_acc: 0.6332 - policy_acc_reg: 1.0000 - value_acc: 0.9855 - value_acc_reg: 0.9586 - val_loss: 3.4300 - val_policy_loss: 3.4334 - val_value_loss: 0.0449 - val_policy_acc: 0.3681 - val_policy_acc_reg: 1.0000 - val_value_acc: 0.9811 - val_value_acc_reg: 0.9551\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.36276\n",
      "Epoch:  20  - lr: 0.0003  - batch: 19  - epoch:  20\n"
     ]
    }
   ],
   "source": [
    "# model.save('models\\\\BughouseNet220620190437.h5')\n",
    "history = model.fit_generator(generator(batch_size,len(train_ids),path=dataset_path), steps_per_epoch=int((len(train_ids)*zip_length)/batch_size), callbacks=callbacks_list,\n",
    "                    epochs=epochs, validation_data=(x_val, [policies_val,values_val]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHistory(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MNist overfitting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResidualNetwork(input_shape):\n",
    "    \n",
    "    channel_pos = 'channels_first'\n",
    "    inp_shape = Input(input_shape,name='input1')\n",
    "    x = Conv2D(256, kernel_size=(3,3), padding = 'same', input_shape=input_shape,data_format=channel_pos,name='conv2d_1')(inp_shape)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_1')(x)\n",
    "    x_a1 = Activation('relu',name='activation_1')(x)\n",
    "\n",
    "    x = Conv2D(256, kernel_size=(3,3),name ='conv2d_2' ,padding = 'same',data_format=channel_pos)(x_a1)\n",
    "    x = BatchNormalization(axis=1, name = 'batch_normalization_2')(x)\n",
    "    x = Activation('relu',name = 'activation_2')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name = 'conv2d_3',padding = 'same',data_format=channel_pos)(x)\n",
    "    x = BatchNormalization(axis=1, name = 'batch_normalization_3')(x)\n",
    "\n",
    "    x = keras.layers.add([x,x_a1],name='add1')\n",
    "    x_a2 = Activation('relu',name='activation_3')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3),name = 'conv2d_4', padding = 'same',data_format=channel_pos)(x_a2)\n",
    "    x = BatchNormalization(axis=1,name = 'batch_normalization_4')(x)\n",
    "    x = Activation('relu',name='activation_4')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name='conv2d_5',padding = 'same',data_format=channel_pos)(x)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_5')(x)\n",
    "\n",
    "    x = keras.layers.add([x,x_a2],name='add_2')\n",
    "    x_a3 = Activation('relu',name='activation_5')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3),name='conv2d_6', padding = 'same',data_format=channel_pos)(x_a3)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_6')(x)\n",
    "    x = Activation('relu',name='activation_6')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name='conv2d_7',padding = 'same',data_format=channel_pos)(x)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_7')(x)\n",
    "\n",
    "    x = keras.layers.add([x,x_a3],name='add_3')\n",
    "    x_a4 = Activation('relu',name='activation_7')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name = 'conv2d_8',padding = 'same',data_format=channel_pos)(x_a4)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_8')(x)\n",
    "    x = Activation('relu',name='activation_8')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name='conv2d_9',padding = 'same',data_format=channel_pos)(x)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_9')(x)\n",
    "\n",
    "    x = keras.layers.add([x,x_a4],name='add4')\n",
    "    x_a5 = Activation('relu',name='activation_9')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3),name='conv2d_10', padding = 'same',data_format=channel_pos)(x_a5)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_10')(x)\n",
    "    x = Activation('relu',name='activation_10')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3),name='conv2d_11', padding = 'same',data_format=channel_pos)(x)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_11')(x)\n",
    "\n",
    "    x = keras.layers.add([x,x_a5],name='add_5')\n",
    "    x_a6 = Activation('relu',name='activation_11')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name='conv2d_12',padding = 'same',data_format=channel_pos)(x_a6)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_12')(x)\n",
    "    x = Activation('relu',name='activation_12')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name='conv2d_13',padding = 'same',data_format=channel_pos)(x)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_13')(x)\n",
    "\n",
    "    x = keras.layers.add([x,x_a6],name='add6')\n",
    "    x_a7 = Activation('relu',name='activation_13')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name='conv2d_14',padding = 'same',data_format=channel_pos)(x_a7)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_14')(x)\n",
    "    x = Activation('relu',name='activation_14')(x)\n",
    "    x = Conv2D(256, kernel_size=(3,3), name='conv2d_15',padding = 'same',data_format=channel_pos)(x)\n",
    "    x = BatchNormalization(axis=1,name='batch_normalization_15')(x)\n",
    "\n",
    "    x = keras.layers.add([x,x_a7],name='add_7')\n",
    "    x_a8 = Activation('relu',name='activation_15')(x)\n",
    "    x = Conv2D(1, kernel_size=(1,1),name='conv2d_17', padding = 'same',data_format=channel_pos)(x_a8)\n",
    "    xb = BatchNormalization(axis=1,name='batch_normalization_17')(x)\n",
    "    xConv = Conv2D(8, kernel_size=(7,7), padding = 'same',name='conv2d_16',data_format=channel_pos)(x_a8)\n",
    "    xA = Activation('relu',name='activation_17')(xb)\n",
    "    xb = BatchNormalization(axis=1,name='batch_normalization_16')(xConv)\n",
    "    xF = Flatten(name='flatten_2')(xA)\n",
    "    xA = Activation('relu',name='activation_16')(xb)\n",
    "\n",
    "    dense_1 = Dense(256, activation='relu',name='dense_1')(xF)\n",
    "    xF = Flatten(name='flatten_1')(xA)\n",
    "\n",
    "    value = Dense(1, activation='tanh', name='value')(dense_1)\n",
    "    policy = Dense(10, activation='softmax', name='policy')(xF)\n",
    "\n",
    "    from keras.models import Model\n",
    "    model = Model(inp_shape, [policy])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchLen:  4680  - DataLen:  60000\n"
     ]
    }
   ],
   "source": [
    "from  LearningRateScheduler import *\n",
    "data_len = len(x_train)\n",
    "epochs = 20\n",
    "batch_len = epochs * int(data_len/ (batch_size))\n",
    "max_lr = 0.001*8\n",
    "total_it = batch_len\n",
    "min_lr = 0.0001\n",
    "print('BatchLen: ', batch_len, ' - DataLen: ', data_len)\n",
    "lr_schedule = OneCycleSchedule(start_lr=max_lr/8, max_lr=max_lr, cycle_length=total_it*.4, cooldown_length=total_it*.6, finish_lr=min_lr)\n",
    "scheduler = LinearWarmUp(lr_schedule, start_lr=min_lr, length=total_it/30)\n",
    "bt = BatchLearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input1 (InputLayer)             (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 28, 1)   64768       input1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 28, 1)   1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 28, 1)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 28, 1)   590080      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 28, 1)   1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 28, 1)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 28, 1)   590080      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256, 28, 1)   1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add1 (Add)                      (None, 256, 28, 1)   0           batch_normalization_3[0][0]      \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256, 28, 1)   0           add1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 28, 1)   590080      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 28, 1)   1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256, 28, 1)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 256, 28, 1)   590080      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256, 28, 1)   1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256, 28, 1)   0           batch_normalization_5[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256, 28, 1)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 256, 28, 1)   590080      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256, 28, 1)   1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 256, 28, 1)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 256, 28, 1)   590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 256, 28, 1)   1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256, 28, 1)   0           batch_normalization_7[0][0]      \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 256, 28, 1)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 256, 28, 1)   590080      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256, 28, 1)   1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 256, 28, 1)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 256, 28, 1)   590080      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256, 28, 1)   1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add4 (Add)                      (None, 256, 28, 1)   0           batch_normalization_9[0][0]      \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 256, 28, 1)   0           add4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 256, 28, 1)   590080      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256, 28, 1)   1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 256, 28, 1)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 28, 1)   590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 256, 28, 1)   1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256, 28, 1)   0           batch_normalization_11[0][0]     \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 256, 28, 1)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 28, 1)   590080      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 256, 28, 1)   1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 256, 28, 1)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 28, 1)   590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 256, 28, 1)   1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add6 (Add)                      (None, 256, 28, 1)   0           batch_normalization_13[0][0]     \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 256, 28, 1)   0           add6[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 28, 1)   590080      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256, 28, 1)   1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 256, 28, 1)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 28, 1)   590080      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 256, 28, 1)   1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 256, 28, 1)   0           batch_normalization_15[0][0]     \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 256, 28, 1)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 2, 28, 1)     514         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2, 28, 1)     8           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 2, 28, 1)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 56)           0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "policy (Dense)                  (None, 10)           570         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,342,340\n",
      "Trainable params: 8,334,656\n",
      "Non-trainable params: 7,684\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "  512/60000 [..............................] - ETA: 9:31 - loss: 2.5902 - acc: 0.0762 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoBray\\.conda\\envs\\TensorFlow_GPU_Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.290199). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 59s 982us/step - loss: 1.3563 - acc: 0.6172 - val_loss: 0.8343 - val_acc: 0.8215\n",
      "Epoch:  1  - lr: 0.001590812  - batch: 0  - epoch:  1\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 54s 901us/step - loss: 0.6064 - acc: 0.8796 - val_loss: 0.4587 - val_acc: 0.9088\n",
      "Epoch:  2  - lr: 0.0033482907  - batch: 1  - epoch:  2\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 54s 903us/step - loss: 0.3410 - acc: 0.9342 - val_loss: 0.3061 - val_acc: 0.9347\n",
      "Epoch:  3  - lr: 0.0051057693  - batch: 2  - epoch:  3\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 54s 902us/step - loss: 0.2215 - acc: 0.9587 - val_loss: 0.2464 - val_acc: 0.9463\n",
      "Epoch:  4  - lr: 0.006863248  - batch: 3  - epoch:  4\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 54s 905us/step - loss: 0.1537 - acc: 0.9743 - val_loss: 0.1996 - val_acc: 0.9525\n",
      "Epoch:  5  - lr: 0.0073792734  - batch: 4  - epoch:  5\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 54s 902us/step - loss: 0.1103 - acc: 0.9848 - val_loss: 0.1812 - val_acc: 0.9554\n",
      "Epoch:  6  - lr: 0.005621795  - batch: 5  - epoch:  6\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 54s 901us/step - loss: 0.0877 - acc: 0.9899 - val_loss: 0.1712 - val_acc: 0.9569\n",
      "Epoch:  7  - lr: 0.0038643163  - batch: 6  - epoch:  7\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 54s 901us/step - loss: 0.0758 - acc: 0.9933 - val_loss: 0.1680 - val_acc: 0.9575\n",
      "Epoch:  8  - lr: 0.0021068377  - batch: 7  - epoch:  8\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 55s 913us/step - loss: 0.0700 - acc: 0.9943 - val_loss: 0.1656 - val_acc: 0.9572\n",
      "Epoch:  9  - lr: 0.0009721154  - batch: 8  - epoch:  9\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 55s 922us/step - loss: 0.0678 - acc: 0.9949 - val_loss: 0.1648 - val_acc: 0.9575\n",
      "Epoch:  10  - lr: 0.0008967949  - batch: 9  - epoch:  10\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 55s 924us/step - loss: 0.0669 - acc: 0.9950 - val_loss: 0.1643 - val_acc: 0.9576\n",
      "Epoch:  11  - lr: 0.00082147436  - batch: 10  - epoch:  11\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 56s 929us/step - loss: 0.0657 - acc: 0.9955 - val_loss: 0.1639 - val_acc: 0.9573\n",
      "Epoch:  12  - lr: 0.0007461538  - batch: 11  - epoch:  12\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 55s 923us/step - loss: 0.0653 - acc: 0.9952 - val_loss: 0.1633 - val_acc: 0.9574\n",
      "Epoch:  13  - lr: 0.00067083334  - batch: 12  - epoch:  13\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 56s 926us/step - loss: 0.0642 - acc: 0.9954 - val_loss: 0.1632 - val_acc: 0.9574\n",
      "Epoch:  14  - lr: 0.0005955128  - batch: 13  - epoch:  14\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 56s 926us/step - loss: 0.0639 - acc: 0.9957 - val_loss: 0.1632 - val_acc: 0.9574\n",
      "Epoch:  15  - lr: 0.0005201923  - batch: 14  - epoch:  15\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 55s 922us/step - loss: 0.0633 - acc: 0.9957 - val_loss: 0.1629 - val_acc: 0.9574\n",
      "Epoch:  16  - lr: 0.00044487178  - batch: 15  - epoch:  16\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 55s 923us/step - loss: 0.0630 - acc: 0.9960 - val_loss: 0.1627 - val_acc: 0.9574\n",
      "Epoch:  17  - lr: 0.00036955127  - batch: 16  - epoch:  17\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 55s 923us/step - loss: 0.0627 - acc: 0.9960 - val_loss: 0.1627 - val_acc: 0.9575\n",
      "Epoch:  18  - lr: 0.00029423076  - batch: 17  - epoch:  18\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 55s 923us/step - loss: 0.0626 - acc: 0.9957 - val_loss: 0.1626 - val_acc: 0.9573\n",
      "Epoch:  19  - lr: 0.00021891025  - batch: 18  - epoch:  19\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 55s 925us/step - loss: 0.0624 - acc: 0.9961 - val_loss: 0.1625 - val_acc: 0.9573\n",
      "Epoch:  20  - lr: 0.00014358974  - batch: 19  - epoch:  20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1842fa75dd8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = getResidualNetwork(input_shape)\n",
    "sgd = optimizers.SGD(lr=0.000, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "def acc_reg(y_true,y_pred):\n",
    "    return K.constant(1) - K.square(K.mean((y_pred-y_true), axis=1))\n",
    "callbacks_list = [bt]\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test),\n",
    "#          callbacks=callbacks_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
